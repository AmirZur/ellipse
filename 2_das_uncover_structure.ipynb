{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DAS**: Uncover structure in a neural network\n",
    "\n",
    "In this notebook, we set up DAS (Distributed Alignment Search) over the internal states of our trained neural network in order to localize high level variables (circularity, color, and/or area).\n",
    "\n",
    "It goes through:\n",
    "1. **Single source DAS**: localizing many variables in one representation\n",
    "2. **Multi-source DAS**: disentangling variables by localizing different variables in different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)\n",
    "# _ = torch.cuda.manual_seed(42) # only if using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single source DAS: localize many variables in a single representation\n",
    "\n",
    "We use DAS to find a representation that mediates the causal effect of one or more high level variables (circularity, color, and/or area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle variables to select which variables to localize & toggle intervention_size to set the # of neurons assigned to the variables\n",
    "variables = [0, 1]\n",
    "intervention_size = 64\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create counterfactual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_train)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load intervenable model with `pyvene`: set up a single intervention over the 1st 64 neurons of the output of the 1st convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "from model_utils import PyTorchCNN\n",
    "from das_utils import CNNConfig\n",
    "\n",
    "# load base model\n",
    "model = PyTorchCNN()\n",
    "model.load_state_dict(torch.load('pytorch_models/amir_cnn_model.pth'))\n",
    "\n",
    "model.config = CNNConfig(\n",
    "    hidden_size=448 # batch x 28 x (28 x 16) -> batch x 28 x 448\n",
    ")\n",
    "\n",
    "intervention_size = 64\n",
    "\n",
    "# create a single intervention on the first 64 neurons of the first convolutional layer\n",
    "representations = [{\n",
    "    \"component\": \"conv1.output\",\n",
    "    \"subspace_partition\": [[0, intervention_size], [intervention_size, model.config.hidden_size]]\n",
    "}]\n",
    "\n",
    "pv_config = pv.IntervenableConfig(\n",
    "    representations=representations,\n",
    "    intervention_types=pv.RotatedSpaceIntervention\n",
    ")\n",
    "pv_model = pv.IntervenableModel(pv_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Epoch 1): 100%|██████████| 40/40 [00:16<00:00,  2.40it/s, loss=0.27] \n",
      "Training (Epoch 2): 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.225]\n",
      "Training (Epoch 3): 100%|██████████| 40/40 [00:16<00:00,  2.42it/s, loss=0.21] \n"
     ]
    }
   ],
   "source": [
    "from das_utils import das_train\n",
    "\n",
    "das_train(pv_model, X_base, X_sources, y_counterfactual, lr=0.0005, num_epochs=3, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate interchange intervention accuracy on a new evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  7.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.859000027179718"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "from das_utils import das_evaluate\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_test)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_test\n",
    ")\n",
    "\n",
    "# evaluate the accuracy of the model on the counterfactual dataset\n",
    "das_evaluate(pv_model, X_base, X_sources, y_counterfactual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-source DAS: localize different variables in different representations\n",
    "\n",
    "We use DAS to find multiple representations that mediate the causal effect of one or more high level variables (circularity, color, and/or area), where each separate representation corresponds to a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle variables to select which variables to localize & toggle intervention_size to set the # of neurons assigned to each variable\n",
    "variables = [0, 1]\n",
    "intervention_size = 64\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create counterfactual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_multi_source_counterfactual_dataset\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_train)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create multi-source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_multi_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load intervenable model with `pyvene`: set up a separate intervention for each variable, but link them to use the same rotation matrix (so they can index different subspaces of the rotated neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "from model_utils import PyTorchCNN\n",
    "from das_utils import CNNConfig\n",
    "\n",
    "# load base model\n",
    "model = PyTorchCNN()\n",
    "model.load_state_dict(torch.load('pytorch_models/amir_cnn_model.pth'))\n",
    "\n",
    "model.config = CNNConfig(\n",
    "    hidden_size=448 # batch x 28 x (28 x 16) -> batch x 28 x 448\n",
    ")\n",
    "\n",
    "intervention_size = 64\n",
    "\n",
    "# create a single intervention on the first 64 neurons of the first convolutional layer\n",
    "representations = [\n",
    "    {\n",
    "        \"component\": \"conv1.output\",\n",
    "        \"subspace_partition\": [[0, intervention_size], [intervention_size, intervention_size * 2], [intervention_size * 2, model.config.hidden_size]],\n",
    "        \"intervention_link_key\": 0 # link interventions to use the same rotation matrix\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"conv1.output\",\n",
    "        \"subspace_partition\": [[intervention_size, intervention_size * 2], [0, intervention_size], [intervention_size * 2, model.config.hidden_size]], \n",
    "        \"intervention_link_key\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "pv_config = pv.IntervenableConfig(\n",
    "    representations=representations,\n",
    "    intervention_types=pv.RotatedSpaceIntervention\n",
    ")\n",
    "pv_model = pv.IntervenableModel(pv_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\tools\\Anaconda3\\envs\\pyvene\\lib\\site-packages\\torch\\_compile.py:24: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n",
      "Training (Epoch 1): 100%|██████████| 40/40 [00:31<00:00,  1.27it/s, loss=0.191]\n",
      "Training (Epoch 2): 100%|██████████| 40/40 [00:32<00:00,  1.23it/s, loss=0.18] \n"
     ]
    }
   ],
   "source": [
    "from das_utils import das_train\n",
    "\n",
    "das_train(pv_model, X_base, X_sources, y_counterfactual, lr=0.0005, num_epochs=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate interchange intervention accuracy on a new evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  3.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8180000185966492"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "from das_utils import das_evaluate\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_test)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_test\n",
    ")\n",
    "\n",
    "# evaluate the accuracy of the model on the counterfactual dataset\n",
    "das_evaluate(pv_model, X_base, X_sources, y_counterfactual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
