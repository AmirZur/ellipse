{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IIT**: Induce structure in a neural network\n",
    "\n",
    "In this notebook, we set up IIT (Interchange Intervention Training) over the internal states of our trained neural network in order to induce the model to localize high level variables (circularity, color, and/or area).\n",
    "\n",
    "It goes through:\n",
    "1. **Single source IIT**: localizing many variables in one representation\n",
    "2. **Multi-source IIT**: disentangling variables by localizing different variables in different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(42)\n",
    "# _ = torch.cuda.manual_seed(42) # only if using GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Single source IIT: localize many variables in a single representation\n",
    "\n",
    "We use IIT to train a neural network to mediate the causal effect of one or more high level variables (circularity, color, and/or area) in a single representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle variables to select which variables to localize & toggle intervention_size to set the # of neurons assigned to the variables\n",
    "variables = [0, 1]\n",
    "intervention_size = 64\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create counterfactual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_train)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load intervenable model with `pyvene`: set up a single intervention over the 1st 64 neurons of the output of the 1st convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "from model_utils import PyTorchCNN\n",
    "from das_utils import CNNConfig\n",
    "\n",
    "# load base model\n",
    "model = PyTorchCNN()\n",
    "model.load_state_dict(torch.load('pytorch_models/amir_cnn_model.pth'))\n",
    "\n",
    "model.config = CNNConfig(\n",
    "    hidden_size=28 # batch x 28 x (28 x 16) -> batch x 28 x 448\n",
    ")\n",
    "\n",
    "intervention_size = 2\n",
    "\n",
    "# create a single intervention on the first 64 neurons of the first convolutional layer\n",
    "representations = [{\n",
    "    \"component\": \"conv1.output\",\n",
    "    \"subspace_partition\": [[0, intervention_size], [intervention_size, model.config.hidden_size]]\n",
    "}]\n",
    "\n",
    "pv_config = pv.IntervenableConfig(\n",
    "    representations=representations,\n",
    "    intervention_types=pv.VanillaIntervention\n",
    ")\n",
    "pv_model = pv.IntervenableModel(pv_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Epoch 1): 100%|██████████| 40/40 [00:03<00:00, 11.71it/s, loss=0.622]\n",
      "Training (Epoch 2): 100%|██████████| 40/40 [00:03<00:00, 11.91it/s, loss=0.6]  \n",
      "Training (Epoch 3): 100%|██████████| 40/40 [00:03<00:00, 12.31it/s, loss=0.602]\n",
      "Training (Epoch 4): 100%|██████████| 40/40 [00:03<00:00, 11.86it/s, loss=0.587]\n",
      "Training (Epoch 5): 100%|██████████| 40/40 [00:03<00:00, 12.01it/s, loss=0.571]\n"
     ]
    }
   ],
   "source": [
    "from das_utils import iit_train\n",
    "\n",
    "iit_train(pv_model, X_base, X_sources, y_counterfactual, lr=0.0005, num_epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate interchange intervention accuracy on a new evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 19.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6890000104904175"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "from das_utils import iit_evaluate\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_test)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_test\n",
    ")\n",
    "\n",
    "# evaluate the accuracy of the model on the counterfactual dataset\n",
    "iit_evaluate(pv_model, X_base, X_sources, y_counterfactual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-source IIT: localize different variables in different representations\n",
    "\n",
    "We use IIT to update multiple representations to mediate the causal effect of one or more high level variables (circularity, color, and/or area), where each separate representation corresponds to a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle variables to select which variables to localize & toggle intervention_size to set the # of neurons assigned to each variable\n",
    "variables = [0, 1]\n",
    "intervention_size = 64\n",
    "n_train = 10000\n",
    "n_test = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create counterfactual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_multi_source_counterfactual_dataset\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_train)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create multi-source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_multi_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load intervenable model with `pyvene`: set up a separate intervention for each variable, but link them to use the same rotation matrix (so they can index different subspaces of the rotated neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "from model_utils import PyTorchCNN\n",
    "from das_utils import CNNConfig\n",
    "\n",
    "# load base model\n",
    "model = PyTorchCNN()\n",
    "model.load_state_dict(torch.load('pytorch_models/amir_cnn_model.pth'))\n",
    "\n",
    "model.config = CNNConfig(\n",
    "    hidden_size=28 # batch x 28 x 28 x 16\n",
    ")\n",
    "\n",
    "intervention_size = 2\n",
    "\n",
    "# create a single intervention on the first 64 neurons of the first convolutional layer\n",
    "representations = [\n",
    "    {\n",
    "        \"component\": \"conv1.output\",\n",
    "        \"subspace_partition\": [[0, intervention_size], [intervention_size, intervention_size * 2], [intervention_size * 2, model.config.hidden_size]],\n",
    "        \"intervention_link_key\": 0 # link interventions to use the same rotation matrix\n",
    "    },\n",
    "    {\n",
    "        \"component\": \"conv1.output\",\n",
    "        \"subspace_partition\": [[intervention_size, intervention_size * 2], [0, intervention_size], [intervention_size * 2, model.config.hidden_size]], \n",
    "        \"intervention_link_key\": 0\n",
    "    }\n",
    "]\n",
    "\n",
    "pv_config = pv.IntervenableConfig(\n",
    "    representations=representations,\n",
    "    intervention_types=pv.VanillaIntervention\n",
    ")\n",
    "pv_model = pv.IntervenableModel(pv_config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training (Epoch 1): 100%|██████████| 40/40 [00:05<00:00,  7.80it/s, loss=0.686]\n",
      "Training (Epoch 2): 100%|██████████| 40/40 [00:05<00:00,  7.81it/s, loss=0.686]\n",
      "Training (Epoch 3): 100%|██████████| 40/40 [00:04<00:00,  8.05it/s, loss=0.687]\n",
      "Training (Epoch 4): 100%|██████████| 40/40 [00:05<00:00,  7.96it/s, loss=0.688]\n",
      "Training (Epoch 5): 100%|██████████| 40/40 [00:04<00:00,  8.11it/s, loss=0.688]\n"
     ]
    }
   ],
   "source": [
    "from das_utils import iit_train\n",
    "\n",
    "iit_train(pv_model, X_base, X_sources, y_counterfactual, lr=0.0005, num_epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate interchange intervention accuracy on a new evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 13.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5249999761581421"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import create_dataset\n",
    "from counterfactual_data_utils import create_single_source_counterfactual_dataset\n",
    "from das_utils import iit_evaluate\n",
    "\n",
    "# first, create base dataset\n",
    "images, labels = create_dataset(n_test)\n",
    "images = images.reshape((-1, 1, 28, 28))\n",
    "coefficients = np.array([0.4, 0.4, 0.4])\n",
    "\n",
    "# create single source counterfactual dataset\n",
    "X_base, X_sources, y_base, y_sources, y_counterfactual = create_single_source_counterfactual_dataset(\n",
    "    variables, images, labels, coefficients, size=n_test\n",
    ")\n",
    "\n",
    "# evaluate the accuracy of the model on the counterfactual dataset\n",
    "iit_evaluate(pv_model, X_base, X_sources, y_counterfactual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvene",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
